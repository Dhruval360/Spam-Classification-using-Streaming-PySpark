{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.datasets import load_files\n",
    "\n",
    "# Text cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./spam/train.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df['Spam/Ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have list of texts, that encoded binary. Using 'decode' is one of possible solutions, but some texts don't allow us to apply decoding. This can be solved by deleting these texts, but because of this, we can lose important information. Instead of this we can do following:\n",
    "\n",
    "* We should remove all special symbols.\n",
    "* Remove 'b' in beginning of each text\n",
    "* Replace all gaps (\\t, \\n, \\r, \\f) between words with spaces\n",
    "* Remove all non-letters characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "#tf = TfidfVectorizer()\n",
    "hvec = HashingVectorizer(n_features=2**9,alternate_sign=False) #***change this logically***\n",
    "\n",
    "def preProcess(X):\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    corpus = []\n",
    "    for i in tqdm(range(0, len(X))):\n",
    "        # Remove special symbols\n",
    "        review = re.sub(r'\\\\r\\\\n', ' ', str(X[i]))\n",
    "        # Remove all symbols except letters\n",
    "        review = re.sub('[^a-zA-Z]', ' ', review)\n",
    "        # Replacing all gaps with spaces \n",
    "        review = re.sub(r'\\s+', ' ', review)                    \n",
    "        # Remove 'b' in the beginning of each text\n",
    "        review = re.sub(r'^b\\s+', '', review)       \n",
    "\n",
    "        review = review.lower().split()\n",
    "        review = [stemmer.stem(word) for word in review if word not in stopwords.words('english')]\n",
    "        review = ' '.join(review)\n",
    "        corpus.append(review)\n",
    "    \n",
    "    # Creating the Bag of Words model\n",
    "    X = hvec.fit_transform(corpus)\n",
    "    return X.toarray()\n",
    "\n",
    "def trainBatch(X, y):\n",
    "    X = preProcess(X)\n",
    "\n",
    "    # Splitting data on train and test dataset\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y,  random_state=9, test_size=0.2)\n",
    "    \n",
    "    global model\n",
    "    model = model.partial_fit(X_train, y_train, np.unique(y))\n",
    "    pred = model.predict(X_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, pos_label = 'spam')\n",
    "    recall = recall_score(y_test, pred, pos_label = 'spam')\n",
    "    conf_m = confusion_matrix(y_test, pred)\n",
    "\n",
    "    print(f\"accuracy: %.3f\" %accuracy)\n",
    "    print(f\"precision: %.3f\" %precision)\n",
    "    print(f\"recall: %.3f\" %recall)\n",
    "    print(f\"confusion matrix: \")\n",
    "    print(conf_m)\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying Naive Bayes model as the likelihood of whether an email is spam or ham is an aposterior probability and they usually show high performance in spam detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "accuracy: 0.979\n",
    "precision: 0.962\n",
    "recall: 1.000\n",
    "confusion matrix: \n",
    "[[452  21]\n",
    " [  0 527]]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeBatch(df, batchSize = 500):\n",
    "    for i in range(len(df)//batchSize):\n",
    "        X, y = df['Message'][i:i + batchSize + 1], df['Spam/Ham'][i:i + batchSize + 1]\n",
    "        X.index = range(batchSize + 1)\n",
    "        y.index = range(batchSize + 1)\n",
    "        yield X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = makeBatch(df, 100)\n",
    "NBatches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ('accuracy', 'precision', 'recall')\n",
    "history = {metric : [] for metric in metrics}\n",
    "for batchNum in range(NBatches):\n",
    "    print(f\"------------------------ Batch {batchNum + 1} ------------------------\")\n",
    "    X,y = next(batch)\n",
    "    scores = trainBatch(X,y)\n",
    "    for i in range(len(metrics)):\n",
    "        history[metrics[i]].append(scores[i])\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for metric in metrics:\n",
    "    plt.plot(history[metric], label = metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.title('Model performance over number of Batches used for training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "model = SGDClassifier(loss='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = makeBatch(df)\n",
    "NBatches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ('accuracy', 'precision', 'recall')\n",
    "history = {metric : [] for metric in metrics}\n",
    "for batchNum in range(NBatches):\n",
    "    print(f\"------------------------ Batch {batchNum + 1} ------------------------\")\n",
    "    X,y = next(batch)\n",
    "    scores = trainBatch(X,y)\n",
    "    for i in range(len(metrics)):\n",
    "        history[metrics[i]].append(scores[i])\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for metric in metrics:\n",
    "    plt.plot(history[metric], label = metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.title('Model performance over number of Batches used for training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "model = Perceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = makeBatch(df)\n",
    "NBatches = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ('accuracy', 'precision', 'recall')\n",
    "history = {metric : [] for metric in metrics}\n",
    "for batchNum in range(NBatches):\n",
    "    print(f\"------------------------ Batch {batchNum + 1} ------------------------\")\n",
    "    X,y = next(batch)\n",
    "    scores = trainBatch(X,y)\n",
    "    for i in range(len(metrics)):\n",
    "        history[metrics[i]].append(scores[i])\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "for metric in metrics:\n",
    "    plt.plot(history[metric], label = metric)\n",
    "\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.ylabel('Metrics')\n",
    "plt.xlabel('Number of Batches')\n",
    "plt.title('Model performance over number of Batches used for training')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = makeBatch(df, 10)\n",
    "NBatches = 5\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "model = MiniBatchKMeans(n_clusters=2, random_state=0, batch_size=6)\n",
    "\n",
    "def trainBatch_cluster(X, y):\n",
    "    X = preProcess(X)\n",
    "    global model\n",
    "    model.partial_fit(X)\n",
    "    pred = model.predict(X)\n",
    "    y_test = [0 if i == \"spam\" else 1 for i in y.tolist()]\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, pred)\n",
    "    precision = precision_score(y_test, pred, pos_label = 0)\n",
    "    recall = recall_score(y_test, pred, pos_label = 0)\n",
    "    conf_m = confusion_matrix(y_test, pred)\n",
    "\n",
    "    print(f\"accuracy: %.3f\" %accuracy)\n",
    "    print(f\"precision: %.3f\" %precision)\n",
    "    print(f\"recall: %.3f\" %recall)\n",
    "    print(f\"confusion matrix: \")\n",
    "    print(conf_m)\n",
    "\n",
    "    return accuracy, precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ('accuracy', 'precision', 'recall')\n",
    "history = {metric : [] for metric in metrics}\n",
    "for batchNum in range(NBatches):\n",
    "    print(f\"------------------------ Batch {batchNum + 1} ------------------------\")\n",
    "    X,y = next(batch)\n",
    "    scores = trainBatch_cluster(X, y)\n",
    "    for i in range(len(metrics)):\n",
    "        history[metrics[i]].append(scores[i])\n",
    "    print(\"---------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96287154c02bd33d964264908876ff144eece4cbcf76f7700a55934c72d2f956"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
